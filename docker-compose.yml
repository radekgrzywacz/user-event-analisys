services:
  db:
    image: postgres:15
    container_name: postgres
    profiles:
      - "all"
      - "dev"
    restart: always
    environment:
      POSTGRES_USER: postgres 
      POSTGRES_PASSWORD: postgres 
      POSTGRES_DB: user_event_generator_db
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./db/initdb:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  migrate-generator:
    image: migrate/migrate
    container_name: migrate-generator
    profiles:
      - "all"
      - "dev"
    volumes:
      - ./synthetic-data-generator/db/schema:/migrations
    command: -source=file:///migrations -database postgres://postgres:postgres@db:5432/user_event_generator_db?sslmode=disable up
    depends_on:
      db:
        condition: service_healthy

  migrate-system:
    image: migrate/migrate
    container_name: migrate-system
    profiles:
      - "all"
      - "dev"
    volumes:
      - ./anomaly-aggregator/db/schema:/migrations
    command: [
      "-path=/migrations",
      "-database=postgres://postgres:postgres@db:5432/user_event_analysis_db?sslmode=disable",
      "up"
    ]
    depends_on:
      db:
        condition: service_healthy

  synthetic-data-generator:
    build:
      context: .
      dockerfile: synthetic-data-generator/Dockerfile
    container_name: synthetic-data-generator
    profiles:
      - "all"
      - "dev"
    depends_on:
      http-ingestor:
        condition: service_healthy
      db:
        condition: service_healthy
      migrate-generator: 
        condition: service_completed_successfully
    env_file:
      - ./synthetic-data-generator/.env.docker
    ports:
      - "18080:8080"
    command: ["./synthetic-data-generator", "-duration", "0", "-concurrency", "2", "-users", "10"]
    
  http-ingestor:
    build: 
      context: .
      dockerfile: http-ingestor/Dockerfile
    container_name: http-ingestor
    profiles:
      - "all"
      - "dev"
    ports:
      - "8081:8081"
    command: ["./http-ingestor"]
    env_file:
      - ./http-ingestor/.env.docker
    healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8081/healthcheck"]
        interval: 5s
        timeout: 3s
        retries: 5
  
  analyser-stat:
    build:
      context: .
      dockerfile: analyser-stat/Dockerfile
    container_name: analyser-stat
    profiles:
      - "all"
    ports: 
      - "8082:8082"
    command: ["./analyser-stat"]
    depends_on:
      - http-ingestor
    env_file:
      - ./analyser-stat/.env.docker

  analyser-ml:
    build:
      context: .
      dockerfile: analyser-ml/Dockerfile
    container_name: analyser-ml
    profiles:
      - "all"
      - "dev"
    depends_on:
      kafka:
        condition: service_started
      init-kafka:
        condition: service_completed_successfully
    environment:
      RUNNING_IN_DOCKER: "1"
      KAFKA_BROKER_ADDRESS: "kafka:29092"
      KAFKA_INPUT_TOPIC: "events"
      KAFKA_OUTPUT_TOPIC: "ml_out"
      KAFKA_CONSUMER_GROUP: "analyser-ml"
      MODEL_PATH: "/models/model.pt"
      SCALER_PATH: "/models/scaler.pkl"
      METRICS_PATH: "/models/metrics.pt"
      STATE_DIR: "/state"
    volumes:
      - ./models:/models
      - ./analyser-ml/state:/state

  anomaly-aggregator:
    build:
      context: .
      dockerfile: anomaly-aggregator/Dockerfile
    container_name: anomaly-aggregator
    profiles:
      - "all"
      - "dev"
    depends_on:
      kafka:
        condition: service_started
      init-kafka:
        condition: service_completed_successfully
      analyser-ml:
        condition: service_started
      analyser-stat:
        condition: service_started
    env_file:
      - ./anomaly-aggregator/.env.docker
    
  ml-trainer:
    build:
      context: .
      dockerfile: ml-trainer/Dockerfile
    container_name: ml-trainer
    profiles:
      - "all"
      - "dev"
    depends_on:
      db:
        condition: service_healthy
    environment:
      RUNNING_IN_DOCKER: "1"
      DATABASE_URL: "postgresql://postgres:postgres@db:5432/user_event_analysis_db"
      MODEL_DIR: "/models"
      TRAINING_STATE_PATH: "/models/training_state.json"
      RETRAIN_MIN_NEW_EVENTS: 2000
      RETRAIN_POLL_INTERVAL_SECONDS: 120
      RETRAIN_MAX_HOURS_BETWEEN_RETRAINS: 6
    volumes:
      - ./models:/models
      
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    container_name: zookeeper
    profiles:
      - "all"
      - "dev"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka
    profiles:
      - "all"
      - "dev"
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    
  init-kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: init-kafka
    profiles:
      - "all"
      - "dev"
    depends_on:
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server kafka:29092 --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic events --replication-factor 1 --partitions 2
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic stat_out --replication-factor 1 --partitions 2
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic ml_out --replication-factor 1 --partitions 2

      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka:29092 --list
      "

  redis:
    image: redis:8.2
    container_name: redis
    profiles:
      - "all"
      - "dev"
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: ["redis-server", "--save", "60", "1", "--loglevel", "warning"]

  grafana:
    image: grafana/grafana:11.3.0
    container_name: grafana
    profiles:
      - "all"
      - "dev"
    ports:
      - "3000:3000"
    depends_on:
      db:
        condition: service_healthy
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"

      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
    
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning


volumes:
  pgdata:
  redis-data:
  grafana-data:
